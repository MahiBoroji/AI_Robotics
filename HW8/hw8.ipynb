{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL IN YOUR NAME AND THE NAME OF YOUR PEER (IF ANY) BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**: Maede Boroji\n",
    "\n",
    "**Peer**: \n",
    "\n",
    "## Collaboration policy\n",
    "Students are responsible for writing their own quizzes, assignments, and exams. For homework assignments, students are welcome (and encouraged) to discuss problems with one peer, **but each student must write their own assignment wrtieup and code individually**. The peer must be listed at the top of the writeup for each assignment. *Note: I will treat AI assistants as peers. That is, students are welcome to discuss problems with an AI assistant, but it is considered cheating to directly obtain an answer by querying the assistant. Please credit any AI assistant that you use.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8 -- Policy gradient learning (100 pts)\n",
    "\n",
    "**Due:** Tuesday, April 22th, 2025 at 11:59 pm\n",
    "\n",
    "*HW credit: This homework is heavily based on UC Berkeley's CS294-112 HW2 from Fall '18.*\n",
    "\n",
    "This homework builds on the material in the slides and Sutton & Barto Chapter Chapter 13.\n",
    "\n",
    "We will use Jupyter/Colab notebooks throughout the semester for writing code and generating assignment outputs.\n",
    "\n",
    "**This homework will be unlike prior homeworks. It will be _entirely_ implementation-based. Some questions will be assessed by running your code, while others will require you to upload trained neural nets, which the grader will evaluate.**\n",
    "\n",
    "## 1) Implementing REINFORCE\n",
    "\n",
    "Recall that the reinforcement learning objective is to learn a $W*$ that maximizes the objective function:\n",
    "$$J(W) = \\mathbb{E}\\left[\\sum_{t}\\gamma^t R(S_t, A_t)\\right]\\enspace.$$\n",
    "\n",
    "The policy gradient approach is to directly take the gradient of this objective:\n",
    "$$ \\nabla J(\\theta) \\propto \\mathbb{E}\\left[G_t \\frac{\\nabla \\pi_W(A_t \\mid S_t)}{\\pi_W(A_t \\mid S_t)}\\right]\\enspace,$$\n",
    "\n",
    "where $G_t=\\sum_{k=t}^T \\gamma^{k-t}R(S_t, A_t)$.\n",
    "\n",
    "In practice, the expectation can be approximated from a batch of $N$ sampled trajectories:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} G_{t,i} \\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "Here we see that the policy $\\pi_W$ is a probability distribution over the action space, conditioned on the state. In the agent-environment loop, the agent sampled an action $A_t$ from $\\pi_W(\\cdot \\mid S_t)$, and the environment responds with a reward $R(S_t, A_t)$. \n",
    "\n",
    "We saw in the lecture that subtracting a baseline that (possibly) depends on the state $S_t$ but not on the action $A_t$ does not change the expectation of the gradient, so we can alternatively compute:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} (G_{t,i}-b(S_{t,i}))\\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "In this assignment, we will implement a value function $V_\\theta^\\pi$ that acts as a state-dependent baseline. The value function is trained to approximate the sum of future rewards starting from a particular state:\n",
    "$$\n",
    "    V_\\theta^\\pi(S_t) \\approx \\sum_{k=t}^T \\mathbb{E}[\\gamma ^{k-t}R(S_k, A_k)]\n",
    "$$\n",
    "\n",
    "\n",
    "In this problem, we will implement the REINFORCE algorithm both with and without a baseline, as well as try a few other techniques to reduce the variance of our gradient estimates (and therefore improve the quality of policy gradient training). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.1) Gaussian policy\n",
    "\n",
    "Your first task is to implement a Gaussian policy class. The class constructor will take as input the following arguments:\n",
    "- `obs_dim`: the number of features in the state\n",
    "- `action_dim`: the dimensionality of the action space\n",
    "- `hid_size`: the number of neurons in the hidden layers\n",
    "- `num_layers`: the number of hidden layers in your network. (*Note: in NN-speak, the \"input\" layer is the actual features, the \"output\" layer is the final set of nodes, and the \"hidden\" layres are all the ones in between.*)\n",
    "\n",
    "The policy will use a neural net based on the arguments described above to compute the mean $\\mu(S_t)$ of a Gaussian, and will use a trainable parameter $\\log\\sigma$ to represent the log of the standard deviation of each dimension of the action space. \n",
    "\n",
    "You can use your `FCNN` function from HW 7 to create your $\\mu$ network **but this time around we will use `nn.Tanh` as the activation function**. You should use `nn.Parameter` to define your trainable (but not state-dependent) log-standard-dev. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "otter": {
     "tests": [
      "q1.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class GaussianPolicy_11(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hid_size, num_layers):\n",
    "        super().__init__()\n",
    "        Layers = nn.ModuleList()\n",
    "        current_dim = obs_dim\n",
    "        for _ in range(num_layers):\n",
    "            Layers.append(nn.Linear(current_dim, hid_size))\n",
    "            Layers.append(nn.Tanh())\n",
    "            current_dim = hid_size\n",
    "        Layers.append(nn.Linear(current_dim, action_dim))\n",
    "        self.mu = nn.Sequential(*Layers)\n",
    "        # Initialize the log std to all zeros\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        ''' \n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        return: mu: Tensor of shape (batch_size, action_dim)\n",
    "                std: Tensor of shape (action_dim,)\n",
    "        '''\n",
    "        mu = self.mu(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mu, std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        '''\n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        return: action: Tensor of shape (batch_size, action_dim)\n",
    "\n",
    "        Hint: Look into torch.distributions.Normal\n",
    "        '''\n",
    "        mu, std = self(obs)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "        return action\n",
    "\n",
    "    def log_prob(self, obs, action):\n",
    "        '''\n",
    "        obs: Tensor of shape (batch_size, obs_dim)\n",
    "        action: Tensor of shape (batch_size, action_dim)\n",
    "        return: log_prob: Tensor of shape (batch_size,)\n",
    "\n",
    "        Hint 1: You may use torch.distributions.Normal.log_prob\n",
    "        Hint 2: Think about how the joint probability of independent variables is \n",
    "        computed and then how you may do this in log space.\n",
    "        '''\n",
    "        mu, std = self(obs)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.2) Sample a trajectory\n",
    "\n",
    "You will now implement a function that samples a collection of trajectories. This function should be nearly identical to the `run_policy` function from HW7, with the difference that it will instead return a dictionary containing the data from the trajectory. \n",
    "\n",
    "I have provided below a `sample_trajectories` function that loops over calls to your `sample_trajectory` function to collect all the data and put it in the format we will need in subsequent functions.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "otter": {
     "tests": [
      "q1.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sample_trajectory_12(env, policy, seed=None):\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    observations, actions, rewards = [], [], []\n",
    "    steps = 0\n",
    "    # ***Your code here:***\n",
    "    # Note: you should discard the final observation (the one that is accompanied by a termination or truncation)\n",
    "    \n",
    "    while True:\n",
    "        action = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0)).numpy().flatten()\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        obs = next_obs\n",
    "        steps += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "\n",
    "    '''\n",
    "    observations: (steps, obs_dim)\n",
    "    actions: (steps, action_dim)\n",
    "    rewards: (steps,) \n",
    "    length: int = steps\n",
    "    '''\n",
    "    observations = np.array(observations, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    path = {'observations': torch.from_numpy(observations),\n",
    "            'actions': torch.from_numpy(actions),\n",
    "            'rewards': torch.from_numpy(rewards),\n",
    "            'length': steps}\n",
    "    return path\n",
    "\n",
    "# DO NOT MODIFY THE CODE BELOW\n",
    "def sample_trajectories_12(env, policy, min_batch_size, seed=None):\n",
    "    timesteps = 0\n",
    "    paths = []\n",
    "    itr = 0\n",
    "    while timesteps < min_batch_size:\n",
    "        with torch.no_grad():   # accelerate computation by turning off unnecessary gradients\n",
    "            path = sample_trajectory_12(env, policy, seed=seed+itr*1000)\n",
    "        itr += 1\n",
    "        paths.append(path)\n",
    "        timesteps += path['length']\n",
    "    return paths, timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.3) Computing the cumulative rewards\n",
    "\n",
    "You will now write code to compute $G_t=\\sum_{k=t}^T \\gamma^{k-t}R(S_t, A_t)$. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "otter": {
     "tests": [
      "q1.3"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sum_of_rewards_13(rewards, gamma):\n",
    "    ''' \n",
    "    rewards: list of torch tensors, each of which is the rewards for a single trajectory\n",
    "    gamma: float, the discount factor\n",
    "    return: torch tensor of shape (num_trajectories * num_steps, ), the reward-to-go for each time step, flattened\n",
    "    '''\n",
    "    all_rewards = []\n",
    "    for trajectory in rewards:\n",
    "        path_length = len(trajectory)\n",
    "        rewards = torch.zeros(path_length)\n",
    "        for t in reversed(range(path_length)):\n",
    "            rewards[t] = trajectory[t]\n",
    "            if t < path_length - 1:\n",
    "                rewards[t] += gamma * rewards[t + 1]\n",
    "        all_rewards.append(rewards)\n",
    "        \n",
    "    all_rewards = torch.cat(all_rewards)\n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.4) Loss function\n",
    "\n",
    "We derived the policy gradient to be:\n",
    "$$\n",
    "    \\nabla J(W) \\approx \\frac{1}{N\\cdot T} \\sum_{i=1}^{N} \\sum_{t=1}^{T} G_{t,i} \\frac{\\nabla\\pi_W(A_{t,i} \\mid S_{t,i})}{\\pi_W(A_{t,i} \\mid S_{t,i})}\\enspace.\n",
    "$$\n",
    "\n",
    "In order to leverage the autodiff capabilities of PyTorch, we need to write a *loss* function to do gradient **descent**. How should the gradient of the loss function relate to the gradient defined above? \n",
    "\n",
    "Write a loss function whose gradient is as desired for gradient descent.\n",
    "\n",
    "(Note: we made a minor change in our derivation to include a factor of $\\frac{1}{T}$. While this theoratically doesn't change anything---we can always scale the learning rate appropriately independently of what multiplicative factor we use---this choice makes implementation easier. Make sure that your loss function incorporates the appropriate scale in order for the autograder to work correctly, and for your learning rate to match the ones suggested in the assignment.)\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q1.4"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def pg_loss_fn_14(policy, observations, actions, returns):\n",
    "    ''' \n",
    "    policy: GaussianPolicy_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    actions: Tensor of shape (num_trajectories * num_steps, action_dim)\n",
    "    returns: Tensor of shape (num_trajectories * num_steps, )\n",
    "    return: loss: Tensor of shape (1, ), a loss whose gradient can be used for gradient *descent*\n",
    "    '''\n",
    "    log_prob = policy.log_prob(observations, actions)\n",
    "    loss = -torch.mean(log_prob * returns)  \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.5) Putting it all together: PG training\n",
    "\n",
    "You will now complete the code below that implements PG training by putting together all the functions you have written so far. \n",
    "\n",
    "The function `train_PG_15` will implement the training loop, which internally calls the function `update_parameters_15`. The latter function takes one gradient descent step using the optimizer.\n",
    "\n",
    "In this question, you will implement one additional trick in `train_PG_15`: normalizing the returns. In particular, if the argument `normalize_returns=True`, your code should normalize the returns to have mean zero and standard deviation one. \n",
    "\n",
    "The parts of the code that you should modify are annotated with `*** 1.5 -- YOUR CODE HERE ***`.\n",
    "\n",
    "*Note: parts of this code are annotated with `*** 2.3 -- YOUR CODE HERE ***`. These parts of the code will not be tested in this question, but in later questions. You may leave those parts of the code unchanged until you reach the relevant problem.*\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q1.5"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def update_parameters_15(policy, observations, actions, returns, optimizer):\n",
    "    # *** 1.5 YOUR CODE HERE ***\n",
    "    loss = pg_loss_fn_14(policy, observations, actions, returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach().item()     # You may print this in your training function for debugging\n",
    "\n",
    "def train_PG_15(env_name, \n",
    "                hid_size=64, \n",
    "                num_layers=2, \n",
    "                use_baseline=False, \n",
    "                num_iterations=100, \n",
    "                batch_size=1000,\n",
    "                gamma=0.99,\n",
    "                normalize_returns=False,\n",
    "                learning_rate=1e-3,\n",
    "                seed=0):\n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Set the random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # *** 1.5 YOUR CODE HERE ***\n",
    "    policy = GaussianPolicy_11(obs_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0], hid_size=hid_size, num_layers=num_layers)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_baseline:\n",
    "        # *** 2.3 YOUR CODE HERE ***\n",
    "        nn_baseline = ...\n",
    "        optimizer_baseline = ...\n",
    "    \n",
    "    total_timesteps = 0\n",
    "    for iter in range(num_iterations):\n",
    "        print(f\"********** Iteration {iter} **********\")\n",
    "\n",
    "        # Sample trajectories\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        paths, timesteps = sample_trajectories_12(env, policy, batch_size, seed=seed+iter*1000)\n",
    "        total_timesteps += timesteps\n",
    "\n",
    "        # Build tensors\n",
    "        observations = torch.cat([path['observations'] for path in paths], dim=0)\n",
    "        actions = torch.cat([path['actions'] for path in paths], dim=0)\n",
    "        rewards = [path['rewards'] for path in paths]\n",
    "\n",
    "        # Print the average undiscounted return\n",
    "        undiscounted_return = torch.cat(rewards).sum() / len(paths)\n",
    "        print(f\"\\tAverage return: {undiscounted_return:.3f}\")\n",
    "\n",
    "        # Compute the reward-to-go\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        rewards_to_go = sum_of_rewards_13(rewards, gamma)\n",
    "\n",
    "        if use_baseline:\n",
    "            # *** 2.3 YOUR CODE HERE ***\n",
    "            baseline = ...\n",
    "            returns = ...\n",
    "        else:\n",
    "            returns = rewards_to_go\n",
    "        if normalize_returns:\n",
    "            # *** 1.5 YOUR CODE HERE ***\n",
    "            # Normalize the returns\n",
    "            # Hint: Add a small value of 1e-5 to the denominator to avoid division by zero\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "        # Update the policy\n",
    "        # *** 1.5 YOUR CODE HERE ***\n",
    "        loss = update_parameters_15(policy, observations, actions, returns, optimizer)\n",
    "        print(f\"\\tPolicy loss: {loss:.3f}\")\n",
    "\n",
    "        if use_baseline:\n",
    "            # *** 2.3 YOUR CODE HERE ***\n",
    "            # Update the baseline\n",
    "            ...\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.6) Experiments\n",
    "\n",
    "Run experiments using `env_name = \"InvertedPendulum-v5\"`. Use the following arguments to your `train_PG` function:\n",
    "- `hid_size`: 64\n",
    "- `num_layers`: 2\n",
    "- `num_iterations`: 100\n",
    "- `batch_size`: ?\n",
    "- `gamma`: 0.99\n",
    "- `normalize_returns`: ?\n",
    "- `learning_rate`: ?\n",
    "- `seed`: 0\n",
    "\n",
    "Find values for the `batch_size`, `normalize_returns`, and `learning_rate` such that the agent achieves the optimal sum of rewards for this environment (1000) in less than 100 iterations. For your choice of hyperparameters, your code should run in just a few seconds --- this limits the `batch_size` that you can choose, or the autograder will time out. Some suggested values to try are included below:\n",
    "- `batch_size in [10, 100, 1000, 3000]`\n",
    "- `learning_rate in [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]`\n",
    "- `normalize_returns in [True, False]`\n",
    "\n",
    "Report your chosen hyperparameters below.\n",
    "\n",
    "The code cell below includes code to visualize your learned policy, which you can use throughout this assignment to observe the behaviors that your learned policy achieves.\n",
    "\n",
    "_Points:_ 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def visualize_policy(env_name, policy, num_episodes=1):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.sample(torch.from_numpy(obs.astype(np.float32)))\n",
    "            action = action.detach().numpy()\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            env.render()\n",
    "            done = terminated or truncated\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "otter": {
     "tests": [
      "q1.6"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size_16 = 1000\n",
    "normalize_returns_16 = True\n",
    "learning_rate_16 = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iteration 0 **********\n",
      "\tAverage return: 7.556\n",
      "\tPolicy loss: -0.079\n",
      "********** Iteration 1 **********\n",
      "\tAverage return: 24.000\n",
      "\tPolicy loss: -0.036\n",
      "********** Iteration 2 **********\n",
      "\tAverage return: 15.111\n",
      "\tPolicy loss: -0.001\n",
      "********** Iteration 3 **********\n",
      "\tAverage return: 26.324\n",
      "\tPolicy loss: -0.023\n",
      "********** Iteration 4 **********\n",
      "\tAverage return: 32.161\n",
      "\tPolicy loss: -0.005\n",
      "********** Iteration 5 **********\n",
      "\tAverage return: 29.303\n",
      "\tPolicy loss: -0.018\n",
      "********** Iteration 6 **********\n",
      "\tAverage return: 31.258\n",
      "\tPolicy loss: -0.003\n",
      "********** Iteration 7 **********\n",
      "\tAverage return: 50.600\n",
      "\tPolicy loss: 0.013\n",
      "********** Iteration 8 **********\n",
      "\tAverage return: 63.812\n",
      "\tPolicy loss: -0.000\n",
      "********** Iteration 9 **********\n",
      "\tAverage return: 90.364\n",
      "\tPolicy loss: -0.030\n",
      "********** Iteration 10 **********\n",
      "\tAverage return: 72.214\n",
      "\tPolicy loss: -0.028\n",
      "********** Iteration 11 **********\n",
      "\tAverage return: 76.071\n",
      "\tPolicy loss: -0.006\n",
      "********** Iteration 12 **********\n",
      "\tAverage return: 83.417\n",
      "\tPolicy loss: 0.018\n",
      "********** Iteration 13 **********\n",
      "\tAverage return: 131.750\n",
      "\tPolicy loss: -0.018\n",
      "********** Iteration 14 **********\n",
      "\tAverage return: 122.600\n",
      "\tPolicy loss: -0.007\n",
      "********** Iteration 15 **********\n",
      "\tAverage return: 144.143\n",
      "\tPolicy loss: -0.029\n",
      "********** Iteration 16 **********\n",
      "\tAverage return: 107.000\n",
      "\tPolicy loss: -0.026\n",
      "********** Iteration 17 **********\n",
      "\tAverage return: 116.300\n",
      "\tPolicy loss: -0.030\n",
      "********** Iteration 18 **********\n",
      "\tAverage return: 221.400\n",
      "\tPolicy loss: -0.000\n",
      "********** Iteration 19 **********\n",
      "\tAverage return: 293.250\n",
      "\tPolicy loss: -0.031\n",
      "********** Iteration 20 **********\n",
      "\tAverage return: 647.667\n",
      "\tPolicy loss: 0.016\n",
      "********** Iteration 21 **********\n",
      "\tAverage return: 565.000\n",
      "\tPolicy loss: 0.017\n",
      "********** Iteration 22 **********\n",
      "\tAverage return: 872.500\n",
      "\tPolicy loss: -0.029\n",
      "********** Iteration 23 **********\n",
      "\tAverage return: 517.500\n",
      "\tPolicy loss: 0.014\n",
      "********** Iteration 24 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.020\n",
      "********** Iteration 25 **********\n",
      "\tAverage return: 602.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 26 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.028\n",
      "********** Iteration 27 **********\n",
      "\tAverage return: 716.500\n",
      "\tPolicy loss: -0.010\n",
      "********** Iteration 28 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.010\n",
      "********** Iteration 29 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.014\n",
      "********** Iteration 30 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.010\n",
      "********** Iteration 31 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.039\n",
      "********** Iteration 32 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.010\n",
      "********** Iteration 33 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.034\n",
      "********** Iteration 34 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.011\n",
      "********** Iteration 35 **********\n",
      "\tAverage return: 136.125\n",
      "\tPolicy loss: 0.024\n",
      "********** Iteration 36 **********\n",
      "\tAverage return: 320.500\n",
      "\tPolicy loss: 0.013\n",
      "********** Iteration 37 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 38 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.020\n",
      "********** Iteration 39 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.026\n",
      "********** Iteration 40 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.036\n",
      "********** Iteration 41 **********\n",
      "\tAverage return: 23.595\n",
      "\tPolicy loss: -0.037\n",
      "********** Iteration 42 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.026\n",
      "********** Iteration 43 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.001\n",
      "********** Iteration 44 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 45 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.032\n",
      "********** Iteration 46 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.002\n",
      "********** Iteration 47 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.031\n",
      "********** Iteration 48 **********\n",
      "\tAverage return: 434.500\n",
      "\tPolicy loss: 0.001\n",
      "********** Iteration 49 **********\n",
      "\tAverage return: 487.667\n",
      "\tPolicy loss: -0.005\n",
      "********** Iteration 50 **********\n",
      "\tAverage return: 413.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 51 **********\n",
      "\tAverage return: 513.500\n",
      "\tPolicy loss: 0.005\n",
      "********** Iteration 52 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.022\n",
      "********** Iteration 53 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.026\n",
      "********** Iteration 54 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.020\n",
      "********** Iteration 55 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.018\n",
      "********** Iteration 56 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.044\n",
      "********** Iteration 57 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.001\n",
      "********** Iteration 58 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.022\n",
      "********** Iteration 59 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.016\n",
      "********** Iteration 60 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.063\n",
      "********** Iteration 61 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.028\n",
      "********** Iteration 62 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.001\n",
      "********** Iteration 63 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.009\n",
      "********** Iteration 64 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.017\n",
      "********** Iteration 65 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.030\n",
      "********** Iteration 66 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.007\n",
      "********** Iteration 67 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.005\n",
      "********** Iteration 68 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.006\n",
      "********** Iteration 69 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.019\n",
      "********** Iteration 70 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.041\n",
      "********** Iteration 71 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.041\n",
      "********** Iteration 72 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.015\n",
      "********** Iteration 73 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.014\n",
      "********** Iteration 74 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.005\n",
      "********** Iteration 75 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.005\n",
      "********** Iteration 76 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.023\n",
      "********** Iteration 77 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.003\n",
      "********** Iteration 78 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.018\n",
      "********** Iteration 79 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.040\n",
      "********** Iteration 80 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.020\n",
      "********** Iteration 81 **********\n",
      "\tAverage return: 199.000\n",
      "\tPolicy loss: -0.036\n",
      "********** Iteration 82 **********\n",
      "\tAverage return: 13.357\n",
      "\tPolicy loss: -0.018\n",
      "********** Iteration 83 **********\n",
      "\tAverage return: 76.615\n",
      "\tPolicy loss: -0.040\n",
      "********** Iteration 84 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.037\n",
      "********** Iteration 85 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.009\n",
      "********** Iteration 86 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.033\n",
      "********** Iteration 87 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.012\n",
      "********** Iteration 88 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.022\n",
      "********** Iteration 89 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.007\n",
      "********** Iteration 90 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.016\n",
      "********** Iteration 91 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.021\n",
      "********** Iteration 92 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 93 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.002\n",
      "********** Iteration 94 **********\n",
      "\tAverage return: 1000.000\n",
      "\tPolicy loss: 0.005\n",
      "********** Iteration 95 **********\n",
      "\tAverage return: 166.000\n",
      "\tPolicy loss: -0.004\n",
      "********** Iteration 96 **********\n",
      "\tAverage return: 71.214\n",
      "\tPolicy loss: -0.043\n",
      "********** Iteration 97 **********\n",
      "\tAverage return: 59.556\n",
      "\tPolicy loss: 0.034\n",
      "********** Iteration 98 **********\n",
      "\tAverage return: 110.556\n",
      "\tPolicy loss: 0.014\n",
      "********** Iteration 99 **********\n",
      "\tAverage return: 78.429\n",
      "\tPolicy loss: 0.001\n"
     ]
    }
   ],
   "source": [
    "#train and visualize the policy\n",
    "# policy_16 = train_PG_15(env_name=\"InvertedPendulum-v5\", \n",
    "#                         hid_size=64, \n",
    "#                         num_layers=2, \n",
    "#                         use_baseline=False, \n",
    "#                         num_iterations=100, \n",
    "#                         batch_size=batch_size_16,\n",
    "#                         gamma=0.99,\n",
    "#                         normalize_returns=normalize_returns_16,\n",
    "#                         learning_rate=learning_rate_16,\n",
    "#                         seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained policy in pt file\n",
    "torch.save(policy_16.state_dict(), \"trained_Pendulum_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\boroj\\miniforge3\\envs\\py310\\lib\\site-packages\\glfw\\__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# visualize_policy(\"InvertedPendulum-v5\", policy_16, num_episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) REINFORCE with baseline\n",
    "\n",
    "We will now implement a value function as a state-dependent neural network baseline. This will allow us to reduce the variance of the gradient computation and hopefully learn to solve significantly harder problems. \n",
    "\n",
    "You will implement the vanilla version, which simply uses $\\mathcal{L}(\\theta) = \\frac{1}{NT}\\sum_{i=1}^N\\sum_{t=1}^T(G_t - V^\\pi_\\theta(s_t))^2$, where $G_t$ is the reward-to-go measured directly from the trajectories.\n",
    "\n",
    "For this, it will be useful to apply the following normalization trick:\n",
    "- During training of $V^\\pi_\\theta$, instead of using the reward-to-go $G_t$ as the target labels, you will first normalize the reward-to-go to have mean zero and standard deviation 1. As we have seen in HW7, this is generally a good idea for NN training. But it is particularly helpful in this case where the scale of the reward-to-go may change over time --- we certainly hope that reward goes up as the agent learns!\n",
    "- When applying the baseline to compute the policy gradient, you will compute the baseline values and scale them so that their mean and standard deviation match those of the reward-to-go in the current batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1) Computing the baseline\n",
    "\n",
    "Your first task is to compute, given a NN baseline $V^\\pi_\\theta$, the baseline value $V_\\theta^\\pi(s_t)$. Make sure that the baseline value is appropriately scaled to have the same mean and standard deviation as the reward-to-go.\n",
    "\n",
    "As usual, you should take care not to divide by zero by adding 1e-5 to any denominator that could go to zero.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q2.1"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_baseline_21(nn_baseline, observations, rewards_to_go):\n",
    "    ''' \n",
    "    nn_baseline: FCNN_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    rewards_to_go: Tensor of shape (num_trajectories * num_steps,)\n",
    "    return: Tensor of shape (num_trajectories * num_steps,) the baseline values computed from nn_baseline, with the same mean and standard dev as returns\n",
    "    '''\n",
    "    with torch.no_grad():   # We don't backgprop through the baseline when computing PGs\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.2) Computing the baseline loss\n",
    "\n",
    "You will now compute the loss function to train the baseline on. This will be the mean squared error of the predictions of the baseline compared to the reward-to-go. To simplify the training, you will normalize the reward-to-go targets to have zero mean and standard deviation one.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q2.2"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def baseline_loss_22(nn_baseline, observations, rewards_to_go):\n",
    "    ''' \n",
    "    nn_baseline: FCNN_11 object\n",
    "    observations: Tensor of shape (num_trajectories * num_steps, obs_dim)\n",
    "    rewards_to_go: Tensor of shape (num_trajectories * num_steps,)\n",
    "    return: singleton Tensor of shape ([]), the mean square error\n",
    "\n",
    "    Hint: Think about the shapes of the output of nn_baseline and the rewards_to_go, and \n",
    "    how this would work if you use broadcasting vs manual reshaping.\n",
    "    '''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.3) Putting it all together: PG training with baseline\n",
    "\n",
    "You will now complete the code below and in the `train_PG_15` function (from Problem 1.5) to incorprate the baseline training into the PG loop. Complete any parts annotated with `*** 2.3 YOUR CODE HERE ***`. \n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q2.3"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def update_baseline_parameters_23(nn_baseline, observations, rewards_to_go, optimizer):\n",
    "    # *** 2.3 YOUR CODE HERE ***\n",
    "    loss = ...\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach().item()     # You may print this in your training function for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.4) Experiments with more complex tasks\n",
    "\n",
    "**Note:** The following tasks will take quite a bit of time to train. Please start early!\n",
    "\n",
    "Run experiments using `env_name = \"HalfCheetah-v5\"`. Use the following arguments to your `train_PG` function:\n",
    "- `hid_size`: 32\n",
    "- `num_layers`: 2\n",
    "- `use_baseline`: ?\n",
    "- `num_iterations`: 100\n",
    "- `batch_size`: ?\n",
    "- `gamma`: 0.95\n",
    "- `normalize_returns`: True\n",
    "- `learning_rate`: ?\n",
    "- `seed`: 0\n",
    "\n",
    "Find values for the `use_baseline`, `batch_size`, and `learning_rate` such that the agent achieves the highest sum of rewards that you can for this environment in less than 100 iterations. Some suggested values to try are included below:\n",
    "- `batch_size in [10000, 30000, 50000]`\n",
    "- `learning_rate in [1e-3, 3e-3, 1e-2, 3e-2, 1e-3]`\n",
    "- `use_baseline in [True, False]`\n",
    "\n",
    "You should be able to obtain sum of rewards above 1800 with an appropriate choice of hyperparameters.\n",
    "\n",
    "Unlike Problem 1.6, this time you should train your policy in a separate notebook and save it as `trained_cheetah_policy.pt`. Because PG training is quite noisy, it is unlikely that the policy at iteration 100 is the best performing one, so you may want to add the following lines to your `train_PG` function in your other notebook:\n",
    "\n",
    "```\n",
    "...\n",
    "total_timesteps = 0\n",
    "highest_returns = -np.inf\n",
    "for iter in range(num_iterations):\n",
    "    ... \n",
    "    undiscounted_return = rewards.sum() / len(paths)\n",
    "    if undiscounted_return > highest_returns:\n",
    "        torch.save(policy.state_dict(), 'trained_cheetah_policy.pt')\n",
    "        highest_returns = undiscounted_return\n",
    "    ...\n",
    "```\n",
    "\n",
    "_Points:_ 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "otter": {
     "tests": [
      "q2.4"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Nothing to do in this code cell. Please run your training and saving code in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Fill out the answers to all questions. Submit a zip file containing hw8.ipynb with your answers and the `trained_cheetah_policy.pt` file you saved to the HW8 assignment on Gradescope. You are free to resubmit as many times as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running your submission against local test cases...\n",
      "\n",
      "\n",
      "Your submission received the following results when run against available test cases:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <p>\n",
       "                        Your submission has been exported. Click\n",
       "                        <a href=\"hw8_2025_04_21T19_09_33_255790.zip\" download=\"hw8_2025_04_21T19_09_33_255790.zip\" target=\"_blank\">here</a> to download\n",
       "                        the zip file.\n",
       "                    </p>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "#grader.export(pdf=False, run_tests=True, files=['trained_cheetah_policy.pt'])\n",
    "grader.export(pdf=False, run_tests=True, files=['trained_Pendulum_policy.pt'])\n",
    "#grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw8",
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.5": {
     "name": "q1.5",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.6": {
     "name": "q1.6",
     "points": 15,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": 20,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
